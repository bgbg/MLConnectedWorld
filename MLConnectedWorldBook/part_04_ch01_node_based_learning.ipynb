{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Node-based learning",
   "id": "886b44c07e83b50b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "## Imports and setup",
   "id": "14f47f411514ffe8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import data_utils\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "fn_coauthors = \"ca-HepPh.txt.gz\"\n",
    "df_coauthors = (\n",
    "    data_utils.load_dataset_from_web(fn_coauthors)\n",
    "    .rename(columns={0: \"source\", 1: \"target\"})\n",
    "    .astype(str)\n",
    ")\n",
    "df_coauthors"
   ],
   "id": "aca48cc17523b413",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Introduction",
   "id": "fb5b648bd96c0195"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "g = nx.from_pandas_edgelist(df_coauthors, \"source\", \"target\")\n",
    "print(\n",
    "    f\"The raw graph has {g.number_of_nodes():,} nodes and {g.number_of_edges():,} edges\"\n",
    ")\n",
    "g = data_utils.get_connected_component_subgraphs(g)[0]\n",
    "print(\n",
    "    f\"The largest connected component that has {g.number_of_nodes():,} nodes and {g.number_of_edges():,} edges\"\n",
    ")"
   ],
   "id": "69dc3d7e95bc08b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# log-log plot of the degree distribution\n",
    "fig, ax = plt.subplots()\n",
    "log_degrees = np.log(np.array([d for n, d in g.degree]))\n",
    "log_counts = np.log(pd.Series(log_degrees).value_counts().sort_index())\n",
    "log_freq = log_counts / log_counts.sum()\n",
    "ax.plot(log_counts.index, log_freq, \".\")\n",
    "fig.suptitle(\"Log-log plot of the degree distribution\\n(the full dataset)\")"
   ],
   "id": "3459eee9bbf66c19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For demonstration purposes, we will use very small percentage of the edges.",
   "id": "547f5694efd67f4b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "df_sample = df_coauthors.sample(frac=0.01, random_state=42)\n",
    "g = nx.from_pandas_edgelist(df_sample, \"source\", \"target\")\n",
    "print(\n",
    "    f\"The raw graph has {g.number_of_nodes():,} nodes and {g.number_of_edges():,} edges\"\n",
    ")\n",
    "g = data_utils.get_connected_component_subgraphs(g)[0]\n",
    "layout = nx.spring_layout(g)\n",
    "nx.draw(g, layout, node_size=10, node_color=\"black\", edge_color=\"gray\", alpha=0.5)\n",
    "print(\n",
    "    f\"We are taking the largest connected component that has {g.number_of_nodes():,} nodes and {g.number_of_edges():,} edges\"\n",
    ")"
   ],
   "id": "1926c75048f1c3e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# log-log plot of the degree distribution\n",
    "fig, ax = plt.subplots()\n",
    "log_degrees = np.log(np.array([d for n, d in g.degree]))\n",
    "log_counts = np.log(pd.Series(log_degrees).value_counts().sort_index())\n",
    "log_freq = log_counts / log_counts.sum()\n",
    "ax.plot(log_counts.index, log_freq, \".\")\n",
    "fig.suptitle(\"Log-log plot of the degree distribution\\n(the 1-percent edge sample)\")"
   ],
   "id": "cdb889f1a9a4d257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The graph that we have here is a graph of co-authorship relationships. The nodes are authors, and the edges are co-authorship relationships. This graph was build by parsing author names from the abstract text, which may itroduce some ambiguity, mainly due to the fact that the same author may be referred to in different ways (\"Gorelik B vs. Gorelik Boris\"). Also, many authors may have the same name, which may lead to the same name being used for different authors. \n",
    "\n",
    "The dataset that we have is anonymous, we will simulate the latter form of ambiguity by scrambling some of the nodes"
   ],
   "id": "d5295002c8f73c75"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# now, we will introduce some abiguity in the graph\n",
    "np.random.seed(42)\n",
    "all_author_ids = set(df_coauthors[\"source\"]).union(set(df_coauthors[\"target\"]))\n",
    "frac_noise = 0.005\n",
    "n_noise = int(frac_noise * len(all_author_ids))\n",
    "\n",
    "scrambled = dict()\n",
    "\n",
    "for i in range(n_noise):\n",
    "    n_to_scramble = np.random.randint(2, 4)\n",
    "    scrambled_name = f\"scrambled_{i:02d}\"\n",
    "    nodes_to_scramble = []\n",
    "    while len(nodes_to_scramble) < n_to_scramble:\n",
    "        node = np.random.choice(list(all_author_ids))\n",
    "        if node not in scrambled:\n",
    "            scrambled[node] = scrambled_name\n",
    "            nodes_to_scramble.append(node)"
   ],
   "id": "2fd5c04b8b230c0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "df_scrambled = df_sample.copy()\n",
    "df_scrambled[\"source\"] = df_scrambled[\"source\"].apply(lambda x: scrambled.get(x, x))\n",
    "df_scrambled[\"target\"] = df_scrambled[\"target\"].apply(lambda x: scrambled.get(x, x))\n",
    "g_scrambled = nx.from_pandas_edgelist(df_scrambled, \"source\", \"target\")\n",
    "# add `scrambled` attribute to the nodes\n",
    "for node in g_scrambled.nodes:\n",
    "    g_scrambled.nodes[node][\"scrambled\"] = node.startswith(\"scrambled\")\n",
    "g_scrambled = data_utils.get_connected_component_subgraphs(g_scrambled)[0]\n",
    "print(\n",
    "    f\"The largest connected component that has {g_scrambled.number_of_nodes():,} nodes and {g_scrambled.number_of_edges():,} edges\"\n",
    ")"
   ],
   "id": "f77e1954291d67d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "layout = nx.spring_layout(g_scrambled)\n",
    "node_colors = [\n",
    "    \"red\" if g_scrambled.nodes[n][\"scrambled\"] else \"black\" for n in g_scrambled.nodes\n",
    "]\n",
    "nx.draw(\n",
    "    g_scrambled,\n",
    "    layout,\n",
    "    node_size=10,\n",
    "    node_color=node_colors,\n",
    "    edge_color=\"gray\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "nx.write_graphml(g_scrambled, \"/Users/boris/temp/graph_scrambled.graphml\")"
   ],
   "id": "bcc82cbfaf15ad93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Building a classifier to distinguish between the original and the scrambled nodes",
   "id": "500eedf6fb2b1369"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To build a classifier that can distinguish between the original and the scrambled nodes, we will use the following features:\n",
    "- the degree of the node\n",
    "- the clustering coefficient of the node\n",
    "- the number of triangles the node is part of\n",
    "- the number of triangles the node is part of that are not part of any other triangle\n",
    "- node centrality measures: betweenness, closeness, eigenvector\n"
   ],
   "id": "ed166d3631c74c5"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_node_features(g) -> pd.DataFrame:\n",
    "    # approx 2 minutes of computation time on the full dataset\n",
    "    degrees = dict(g.degree)\n",
    "    clustering = nx.clustering(g)\n",
    "    triangles = nx.triangles(g)\n",
    "    triangles_no_others = dict()\n",
    "    n_depth_2 = dict()\n",
    "    n_depth_4 = dict()\n",
    "    for node in tqdm(g.nodes, total=len(g.nodes), leave=False):\n",
    "        triangles_no_others[node] = sum(\n",
    "            [1 for n in g.neighbors(node) if n in triangles and triangles[n] == 1]\n",
    "        )\n",
    "        n_depth_2[node] = len(\n",
    "            set(nx.single_source_shortest_path_length(g, node, cutoff=2))\n",
    "        )\n",
    "        n_depth_4[node] = len(\n",
    "            set(nx.single_source_shortest_path_length(g, node, cutoff=4))\n",
    "        )\n",
    "    ret = pd.DataFrame(\n",
    "        {\n",
    "            \"degree\": degrees,\n",
    "            \"clustering\": clustering,\n",
    "            \"triangles\": triangles,\n",
    "            \"triangles_no_others\": triangles_no_others,\n",
    "            \"n_depth_2\": n_depth_2,\n",
    "            \"n_depth_4\": n_depth_4,\n",
    "        }\n",
    "    )\n",
    "    return ret\n",
    "\n",
    "\n",
    "# df_node_features = compute_node_features(g_scrambled)"
   ],
   "id": "9d19da43fd460d4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Centrality features\n",
    "\n",
    "Node centrality is a measure of the importance or influence of a node within a network. There are several types of centrality measures, such as:\n",
    "\n",
    "- **Degree Centrality**: The number of direct connections a node has.\n",
    "- **Closeness Centrality**: How close a node is to all other nodes in the network.\n",
    "- **Betweenness Centrality**: The number of times a node acts as a bridge along the shortest path between two other nodes.\n",
    "- **Eigenvector Centrality**: The influence of a node based on the influence of its neighbors.\n",
    "- **Katz Centrality**: A measure that considers the number of all paths that lead to a node, attenuated by the length of the paths.\n",
    "- **PageRank**: A measure that evaluates the importance of nodes based on the structure of incoming links.\n",
    "- **HITS (Hubs and Authorities)**: Identifies two types of nodes: hubs, which point to many nodes, and authorities, which are pointed to by many hubs.\n",
    "- **Clustering Coefficient**: The degree to which nodes tend to cluster together.\n",
    "\n",
    "\n",
    "Note, that some of these metrics can be very computationally expensive, especially when computed directly, without any approximation heuristics.  \n",
    "\n",
    "\n",
    "Look at the description of the betweenness centrality. It is a logical assumption that a node that represents multiple actual authors will have a higher betweenness centrality than a node that represents a single author. This is because the former will be part of many shortest paths between other nodes, while the latter will be part of fewer shortest paths. For the same reason, the closeness centrality of a node that represents multiple authors will be lower than the closeness centrality of a node that represents a single author.\n"
   ],
   "id": "dc8ce9c77be2b517"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_centrality_features(g) -> pd.DataFrame:\n",
    "    # approx 8 minutes of computation time on the full dataset.\n",
    "    # 1 minute on a 1-percent edge sample\n",
    "    betweenness_centrality = nx.betweenness_centrality(g)\n",
    "    closeness_centrality = nx.closeness_centrality(g)\n",
    "    pagerank = nx.pagerank(g)\n",
    "    ret = pd.DataFrame(\n",
    "        {\n",
    "            \"betweenness_centrality\": betweenness_centrality,\n",
    "            \"closeness_centrality\": closeness_centrality,\n",
    "            \"katz\": nx.katz_centrality(g),\n",
    "        }\n",
    "    )\n",
    "    return ret\n",
    "\n",
    "\n",
    "df_centrality_features = compute_centrality_features(g_scrambled)"
   ],
   "id": "c5a8e98fea99a560",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Community features\n",
    "\n",
    "We may think of a community as a group of nodes that are more connected to each other than to the rest of the network. Community detection is the process of identifying these groups of nodes. These algorithms rely on various parameters such as resolution and are stochastic in their nature. Thus, we may assume that if a single node represents multiple authors, it is more likely that its community assignment will be stronger influenced by the changes in parameters. \n",
    "\n",
    "\n",
    "A good reading on community detection algorithms can be found here: https://memgraph.com/blog/community-detection-algorithms-with-python-networkx\n"
   ],
   "id": "c5dee71d8e404efe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Louvain community detection algorithm\n",
    "\n",
    "We will use Louvain community detection algorithm to detect communities in the graph. The Louvain algorithm is a popular community detection algorithm that optimizes **Modularity**.\n",
    " \n",
    "Modularity is a measure of the quality of a partition of a network into communities. It compares the number of edges within a community to the expected number of edges in a random network with the same degree distribution. The Louvain algorithm is based on the concept of modularity optimization, which is a measure of the quality of a partition of a network into communities. \n",
    "\n",
    "The algorithm works by iteratively optimizing the modularity of the network by moving nodes between communities.  The **resolution** parameter of the Louvain algorithm controls the trade-off between the size of the communities and the number of communities. A larger resolution parameter will result in fewer, larger communities, while a smaller resolution parameter will result in more, smaller communities."
   ],
   "id": "a08498c6eaa5a688"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# community detection using louvain algorithm\n",
    "fig, ax = plt.subplots()\n",
    "for res in tqdm([0.1, 0.5, 1, 4]):\n",
    "    communities = nx.community.louvain_communities(g_scrambled, resolution=res)\n",
    "    sizes = [len(c) for c in communities]\n",
    "    sizes.sort(key=lambda x: -x)\n",
    "    ax.plot(range(len(sizes)), sizes, \".\", label=f\"resolution={res}\")\n",
    "    # assign `community_{res}` attribute to the nodes\n",
    "    for i, c in enumerate(communities):\n",
    "        for n in c:\n",
    "            g_scrambled.nodes[n][\n",
    "                f\"community_{res:.2f}\"\n",
    "            ] = f\"community@{res:.2f}_{i:03d}\"\n",
    "ax.legend()\n",
    "ax.set_yscale(\"log\")"
   ],
   "id": "8ed1062f177e9d72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What we see is that the larger the `resolution` parameter of the Louvain algorithm, the larger the communities that are detected. This is because the resolution parameter controls the trade-off between the size of the communities and the number of communities. A larger resolution parameter will result in fewer, larger communities, while a smaller resolution parameter will result in more, smaller communities.\n",
    "\n",
    "\n",
    "We will use this use this information to count the fraction of neighbors that are in the same community as the node itself. We expect that scrambled nodes will have a lower fraction of neighbors in the same community.\n",
    " \n",
    "\n",
    "We will repeat this for different values of the resolution parameter and use this as a feature for the classifier."
   ],
   "id": "a277d328d6f6321e"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_community_features(g) -> pd.DataFrame:\n",
    "    # about 1 minute of computation time\n",
    "    resolutions = np.round(np.linspace(0.01, 4, 10), 2)\n",
    "    for res in tqdm(resolutions):\n",
    "        communities = nx.community.louvain_communities(g, resolution=res)\n",
    "        sizes = [len(c) for c in communities]\n",
    "        sizes.sort(key=lambda x: -x)\n",
    "        # assign `community_{res}` attribute to the nodes\n",
    "        for i, c in enumerate(communities):\n",
    "            for n in c:\n",
    "                g.nodes[n][f\"community_{res:.2f}\"] = f\"community@{res:.2f}_{i:03d}\"\n",
    "\n",
    "    ret = []\n",
    "    for n in g.nodes:\n",
    "        neighbors = set(g.neighbors(n))\n",
    "        curr = {\"node_id\": n}\n",
    "        for res in resolutions:\n",
    "            community_this = g.nodes[n][f\"community_{res:.2f}\"]\n",
    "            neighbors_in_community = [\n",
    "                g.nodes[nb][f\"community_{res:.2f}\"] == community_this\n",
    "                for nb in neighbors\n",
    "            ]\n",
    "            neighboring_communities = set(\n",
    "                [g.nodes[nb][f\"community_{res:.2f}\"] for nb in neighbors]\n",
    "            )\n",
    "            n_neighboring_communities = len(neighboring_communities)\n",
    "            curr[f\"same_community@{res:.2f}\"] = sum(neighbors_in_community) / len(\n",
    "                neighbors\n",
    "            )\n",
    "            curr[f\"n_neighboring_communities@{res:.2f}\"] = n_neighboring_communities\n",
    "        ret.append(curr)\n",
    "    df_community_features = pd.DataFrame(ret).set_index(\"node_id\")\n",
    "    cols_same = [c for c in df_community_features.columns if \"same_community\" in c]\n",
    "    cols_neighboring = [\n",
    "        c for c in df_community_features.columns if \"n_neighboring\" in c\n",
    "    ]\n",
    "    mean_same = df_community_features[cols_same].mean(axis=1)\n",
    "    mean_neighboring = df_community_features[cols_neighboring].mean(axis=1)\n",
    "    df_community_features[\"mean_same\"] = mean_same\n",
    "    df_community_features[\"mean_neighboring\"] = mean_neighboring\n",
    "    for perc in [10, 25, 75, 90]:\n",
    "        df_community_features[f\"percentile_same_{perc}\"] = df_community_features[\n",
    "            cols_same\n",
    "        ].apply(lambda x: np.percentile(x, perc), axis=1)\n",
    "        df_community_features[f\"percentile_neighboring_{perc}\"] = df_community_features[\n",
    "            cols_neighboring\n",
    "        ].apply(lambda x: np.percentile(x, perc), axis=1)\n",
    "\n",
    "    return df_community_features\n",
    "\n",
    "\n",
    "df_community_features = compute_community_features(g_scrambled)"
   ],
   "id": "2b871bb4ec9b671a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ego-graph related features\n",
    "\n",
    "An **ego graph** is a subgraph that includes a central node and all of its neighbors. Ego graphs are useful for analyzing the local structure of a network around a specific node. Sometimes it is intersting to examine the effect of removing the central node from the ego graph. We may assume that the effect of such a removal is stronger for nodes that represent multiple authors, than for nodes that represent a single author.\n",
    "\n",
    "In the most obvious case, when a scrambled node connects two unrelated parts of a graph, removing it will disconnect the graph. But other, more subtle effects are possible, such as changes in the clustering coefficient, the diameter of the graph, or the number of connected components."
   ],
   "id": "983ff30fd5ccc8be"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_ego_graph_features(g):\n",
    "    vals = []\n",
    "    for n in tqdm(g.nodes, desc=\"Ego-related features\", total=len(g.nodes)):\n",
    "        ego = nx.ego_graph(g, n, radius=1)\n",
    "        ego_no_self = ego.copy()\n",
    "        ego_no_self.remove_node(n)\n",
    "\n",
    "        curr = dict(node_id=n)\n",
    "        betweenness_ego = nx.betweenness_centrality(ego)\n",
    "        curr[\"betweenness_ego\"] = betweenness_ego[n]\n",
    "        closeness_ego = nx.closeness_centrality(ego)\n",
    "        curr[\"closeness_ego\"] = closeness_ego[n]\n",
    "        clustering_before = nx.average_clustering(ego)\n",
    "        clustering_after = nx.average_clustering(ego_no_self)\n",
    "        curr[\"delta_clustering_ego\"] = clustering_after - clustering_before\n",
    "\n",
    "        diameter_before = nx.diameter(ego)\n",
    "\n",
    "        try:\n",
    "            diameter_after = nx.diameter(ego_no_self)\n",
    "        except nx.NetworkXError:\n",
    "            # if the graph is not connected, the diameter is infinite\n",
    "            # we'll set it to the number of nodes + 1\n",
    "            diameter_after = len(ego_no_self.nodes) + 1\n",
    "\n",
    "        curr[\"delta_diameter_ego\"] = diameter_after - diameter_before\n",
    "\n",
    "        curr[\"n_components_no_self_ego\"] = nx.number_connected_components(ego_no_self)\n",
    "        density_before = nx.density(ego)\n",
    "        density_after = nx.density(ego_no_self)\n",
    "        curr[\"delta_density_ego\"] = density_after - density_before\n",
    "        vals.append(curr)\n",
    "    return pd.DataFrame(vals).set_index(\"node_id\")\n",
    "\n",
    "\n",
    "df_egograph_features = compute_ego_graph_features(g_scrambled)"
   ],
   "id": "a1fe66f350c1001d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Supervised learning\n",
    "\n",
    "Now, we have computed the features that we will use to build a classifier that can distinguish between the original and the scrambled nodes. We will use the following features:"
   ],
   "id": "5d621a8140def6ae"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "df_all = pd.concat(\n",
    "    [df_egograph_features, df_centrality_features, df_community_features], axis=1\n",
    ")\n",
    "y = np.array([\"scrambled\" in i for i in df_community_features.index])\n",
    "print(f\"{df_all.shape=}, {y.mean()=:.3f}\")"
   ],
   "id": "fddb9179f36dc6c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Our dataset is highly imbalanced. To force the classifier to pay more attention to the minority class, we will use sample weights. We will set the sample weight to 100 for the minority class and 1 for the majority class.",
   "id": "7faae850b012a116"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "weights = np.ones_like(y, dtype=float)\n",
    "weights[y] = 100"
   ],
   "id": "a7f4490404488e30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "    df_all, y, weights, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize the CatBoost classifier\n",
    "classifier = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=1e-4,\n",
    "    depth=8,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    early_stopping_rounds=150,  # Early stopping\n",
    "    l2_leaf_reg=10,  # L2 regularization\n",
    "    subsample=0.8,  # Subsample fraction\n",
    "    verbose=True,\n",
    ")\n",
    "# Fit the model with sample weights\n",
    "classifier.fit(\n",
    "    X_train, y_train, sample_weight=weights_train, eval_set=(X_test, y_test), plot=True\n",
    ")"
   ],
   "id": "260c3d98bb7151bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def show_prediction_performance(X_test, y_test, classifier):\n",
    "    # Predict and evaluate if needed\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    p_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    print(f\"Matthews correlation coefficient: {mcc:.2f}\")\n",
    "    precision = 100 * np.sum(y_test & y_pred) / np.sum(y_pred)\n",
    "    recall = 100 * np.sum(y_test & y_pred) / np.sum(y_test)\n",
    "    print(\n",
    "        f\"Of the {y_pred.sum():3,d} nodes predicted as scrambled,      {precision:.1f}% are actually scrambled\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Of the {y_test.sum():,d}  nodes that are actually scrambled, {recall:.1f}% are predicted as scrambled\"\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=2, figsize=(9, 6), height_ratios=[5, 1], width_ratios=[5, 4]\n",
    "    )\n",
    "    ax = axes[0, 0]\n",
    "    # roc curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, p_pred_proba)\n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    ax.set_xlabel(\"False positive rate\")\n",
    "    ax.set_ylabel(\"True positive rate\")\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    ax.set_title(f\"AUC: {auc_score:.2f}\", va=\"top\")\n",
    "\n",
    "    ax = axes[1, 0]\n",
    "    lix = np.argsort(p_pred_proba)\n",
    "    ax.plot(p_pred_proba[lix], \".\", ms=0.5)\n",
    "    y_test_sorted = y_test[lix]\n",
    "    for i in range(len(y_test_sorted)):\n",
    "        if y_test_sorted[i]:\n",
    "            ax.axvline(i, color=\"red\", alpha=0.4)\n",
    "    ax.axhline(0.5, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    # feature importance\n",
    "    try:\n",
    "        feature_importance = (\n",
    "            classifier.get_feature_importance(prettified=True).head(10).iloc[::-1]\n",
    "        )\n",
    "        feature_importance.plot.barh(\n",
    "            x=\"Feature Id\", y=\"Importances\", legend=False, ax=ax\n",
    "        )\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set_xlabel(\"Feature Importance\")\n",
    "    except:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    # turn off\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout()"
   ],
   "id": "423105c73ea682ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "show_prediction_performance(X_test, y_test, classifier)\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\"Prediction performance on the partial dataset\", y=1.02)"
   ],
   "id": "be3eb326e55be7b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Analyzing the full dataset\n",
    "Now, let's load the full dataset"
   ],
   "id": "17b0843345f1493b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "df_full = data_utils.load_dataset_from_local(\"df_node_disambiguation_task\").set_index(\n",
    "    \"node_id\", drop=True\n",
    ")\n",
    "df_full.shape"
   ],
   "id": "315a891ce5349793",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "y = np.array([\"scrambled\" in i for i in df_full.index])\n",
    "weights = np.ones_like(y, dtype=float)\n",
    "weights[y] = 100\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "    df_full, y, weights, test_size=0.2, random_state=42\n",
    ")\n",
    "classifier = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=1e-5,\n",
    "    depth=12,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    early_stopping_rounds=150,  # Early stopping\n",
    "    l2_leaf_reg=10,  # L2 regularization\n",
    "    subsample=0.8,  # Subsample fraction\n",
    "    verbose=True,\n",
    ")\n",
    "classifier.fit(\n",
    "    X_train, y_train, sample_weight=weights_train, eval_set=(X_test, y_test), plot=True\n",
    ")"
   ],
   "id": "bac935e6d2b76aef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "show_prediction_performance(X_test, y_test, classifier)",
   "id": "b8ef20b69842df0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### SHAP Values for Better Interpretability\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values provide a unified measure of feature importance. They are based on cooperative game theory and offer a way to explain the output of any machine learning model. SHAP values can be used to explain the prediction of any instance in terms of the contribution of each feature to the prediction.\n",
    "\n",
    "SHAP values are computed by first considering all possible feature combinations and then calculating the contribution of each feature across these combinations. The values of the SHAP values represent the average contributions of the features to the prediction. SHAP values can be positive or negative, depending on whether the feature positively or negatively contributes to the prediction. The larger the absolute value of the SHAP value, the greater the feature's contribution to the prediction."
   ],
   "id": "74a125f68386e2d7"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import shap\n",
    "\n",
    "shap_values = shap.TreeExplainer(classifier).shap_values(X_test)"
   ],
   "id": "3a3ea5f4e2f52006",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "shap.summary_plot(shap_values, X_test, max_display=10)",
   "id": "a52808f65ecd47e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You might be unimpressed by the performance of the classifier. However, consider that the only informtion that the classifier has is the structure of the graph. The classifier does not know the names of the nodes, the content of the abstracts, or any other information that could be used to distinguish between the original and the scrambled nodes. The fact that the classifier can distinguish between the original and the scrambled nodes with an AUC of 0.85 is quite impressive.\n",
    "\n",
    "Compare the performance of the classifier with the performance of a majority class classifier and a random classifier."
   ],
   "id": "870dc39e574b5af0"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# for the purpose of performance comparison, we will use a majority class classifier and a random classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train, sample_weight=weights_train)\n",
    "show_prediction_performance(X_test, y_test, dummy)\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\"Majority class classifier\", y=1.02)\n",
    "dummy = DummyClassifier(strategy=\"prior\")\n",
    "dummy.fit(X_train, y_train, sample_weight=weights_train)\n",
    "show_prediction_performance(X_test, y_test, dummy)\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\"Random classifier\", y=1.02)"
   ],
   "id": "b7f3839ebfb42800",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a1347ebff13eee9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
