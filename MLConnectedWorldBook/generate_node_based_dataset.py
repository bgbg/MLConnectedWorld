"""
Utility File for Generating the Full Dataset for the Node-Based Learning Chapter

This Python file contains functions to generate the full dataset required for the node-based learning chapter. The main function, `compute_ego_graph_features`, computes various ego network features for each node in a graph and returns the results as a DataFrame. The dataset generated by this script includes features such as betweenness centrality, closeness centrality, changes in clustering coefficient, diameter, number of components, and density for each node's ego network.

The actual dataset used in the node-based learning chapter is provided in the repository, located in the 'data' folder. This file is provided to ensure repeatability and transparency in how the dataset was generated.

Usage:
    - Ensure you have the required libraries installed: pandas, networkx, tqdm, joblib
    - Place this script in the same directory as your graph data or adjust the paths accordingly
    - Run the script to generate the dataset and save it as needed

The script is designed to utilize parallel processing to speed up the computation, leveraging 8 CPUs.

Author: Boris Gorelik

"""

import os
import sys

sys.path.append(os.getcwd())
from tqdm.auto import tqdm
import numpy as np
import data_utils


from joblib import Parallel, delayed
import pandas as pd
import networkx as nx
from tqdm import tqdm


def compute_node_features(g) -> pd.DataFrame:
    # approx 2 minutes of computation time on the full dataset
    degrees = dict(g.degree)
    clustering = nx.clustering(g)
    triangles = nx.triangles(g)
    triangles_no_others = dict()
    n_depth_2 = dict()
    n_depth_4 = dict()
    for node in tqdm(g.nodes, total=len(g.nodes), leave=False, desc="Node features"):
        triangles_no_others[node] = sum(
            [1 for n in g.neighbors(node) if n in triangles and triangles[n] == 1]
        )
        n_depth_2[node] = len(
            set(nx.single_source_shortest_path_length(g, node, cutoff=2))
        )
        n_depth_4[node] = len(
            set(nx.single_source_shortest_path_length(g, node, cutoff=4))
        )
    ret = pd.DataFrame(
        {
            "degree": degrees,
            "clustering": clustering,
            "triangles": triangles,
            "triangles_no_others": triangles_no_others,
            "n_depth_2": n_depth_2,
            "n_depth_4": n_depth_4,
        }
    )
    return ret


def _get_betweenness_centrality(g):
    return nx.betweenness_centrality(g)


def _get_closeness_centrality(g):
    return nx.closeness_centrality(g)


def compute_centrality_features(g) -> pd.DataFrame:
    # approx 8 minutes of computation time on the full dataset.
    # 1 minute on a 1-percent edge sample
    betweenness_centrality = _get_betweenness_centrality(g)
    closeness_centrality = _get_closeness_centrality(g)
    ret = pd.DataFrame(
        {
            "betweenness_centrality": betweenness_centrality,
            "closeness_centrality": closeness_centrality,
            # "katz": nx.katz_centrality(g),
        }
    )
    return ret


def compute_community_features(g) -> pd.DataFrame:
    # about 1 minute of computation time
    resolutions = np.round(np.linspace(0.01, 4, 10), 2)
    for res in tqdm(resolutions, desc="Community features"):
        communities = nx.community.louvain_communities(g, resolution=res)
        sizes = [len(c) for c in communities]
        sizes.sort(key=lambda x: -x)
        # assign `community_{res}` attribute to the nodes
        for i, c in enumerate(communities):
            for n in c:
                g.nodes[n][f"community_{res:.2f}"] = f"community@{res:.2f}_{i:03d}"

    ret = []
    for n in g.nodes:
        neighbors = set(g.neighbors(n))
        curr = {"node_id": n}
        for res in resolutions:
            community_this = g.nodes[n][f"community_{res:.2f}"]
            neighbors_in_community = [
                g.nodes[nb][f"community_{res:.2f}"] == community_this
                for nb in neighbors
            ]
            neighboring_communities = set(
                [g.nodes[nb][f"community_{res:.2f}"] for nb in neighbors]
            )
            n_neighboring_communities = len(neighboring_communities)
            curr[f"same_community@{res:.2f}"] = sum(neighbors_in_community) / len(
                neighbors
            )
            curr[f"n_neighboring_communities@{res:.2f}"] = n_neighboring_communities
        ret.append(curr)
    df_community_fraction = pd.DataFrame(ret).set_index("node_id")
    cols_same = [c for c in df_community_fraction.columns if "same_community" in c]
    cols_neighboring = [
        c for c in df_community_fraction.columns if "n_neighboring" in c
    ]
    mean_same = df_community_fraction[cols_same].mean(axis=1)
    mean_neighboring = df_community_fraction[cols_neighboring].mean(axis=1)
    df_community_fraction["mean_same"] = mean_same
    df_community_fraction["mean_neighboring"] = mean_neighboring
    for perc in [10, 25, 75, 90]:
        df_community_fraction[f"percentile_same_{perc}"] = df_community_fraction[
            cols_same
        ].apply(lambda x: np.percentile(x, perc), axis=1)
        df_community_fraction[f"percentile_neighboring_{perc}"] = df_community_fraction[
            cols_neighboring
        ].apply(lambda x: np.percentile(x, perc), axis=1)

    return df_community_fraction


def compute_ego_graph_features_node(g, n):
    ego = nx.ego_graph(g, n, radius=1)
    ego_no_self = ego.copy()
    ego_no_self.remove_node(n)

    curr = dict(node_id=n)
    betweenness_ego = nx.betweenness_centrality(ego)
    curr["betweenness_ego"] = betweenness_ego[n]
    closeness_ego = nx.closeness_centrality(ego)
    curr["closeness_ego"] = closeness_ego[n]
    clustering_before = nx.average_clustering(ego)
    if len(ego_no_self.nodes) == 0:
        clustering_after = 0
    else:
        clustering_after = nx.average_clustering(ego_no_self)
    curr["delta_clustering_ego"] = clustering_after - clustering_before

    diameter_before = nx.diameter(ego)
    if not diameter_before:
        diameter_after = 0
    else:
        try:
            diameter_after = nx.diameter(ego_no_self)
        except nx.NetworkXError:
            diameter_after = len(ego_no_self.nodes) + 1

    curr["delta_diameter_ego"] = diameter_after - diameter_before

    curr["n_components_no_self_ego"] = nx.number_connected_components(ego_no_self)
    density_before = nx.density(ego)
    density_after = nx.density(ego_no_self)
    curr["delta_density_ego"] = density_after - density_before

    return curr


def compute_ego_graph_features(g):
    results = Parallel(n_jobs=8)(
        delayed(compute_ego_graph_features_node)(g, n)
        for n in tqdm(g.nodes, desc="Ego-related features", total=len(g.nodes))
    )
    return pd.DataFrame(results).set_index("node_id")


def get_graph():
    fn_coauthors = "ca-HepPh.txt.gz"
    df_coauthors = (
        data_utils.load_dataset_from_web(fn_coauthors)
        .rename(columns={0: "source", 1: "target"})
        .astype(str)
    )

    np.random.seed(42)
    all_author_ids = set(df_coauthors["source"]).union(set(df_coauthors["target"]))
    frac_noise = 0.005
    n_noise = int(frac_noise * len(all_author_ids))

    scrambled_full = dict()

    for i in range(n_noise):
        n_to_scramble = np.random.randint(2, 4)
        scrambled_name = f"scrambled_{i:02d}"
        nodes_to_scramble = []
        while len(nodes_to_scramble) < n_to_scramble:
            node = np.random.choice(list(all_author_ids))
            if node not in scrambled_full:
                scrambled_full[node] = scrambled_name
                nodes_to_scramble.append(node)
    df_coauthors_full_scrambled = df_coauthors.copy()
    df_coauthors_full_scrambled["source"] = df_coauthors_full_scrambled["source"].apply(
        lambda x: scrambled_full.get(x, x)
    )
    df_coauthors_full_scrambled["target"] = df_coauthors_full_scrambled["target"].apply(
        lambda x: scrambled_full.get(x, x)
    )
    ret = nx.from_pandas_edgelist(df_coauthors_full_scrambled, "source", "target")
    # add `scrambled` attribute to the nodes
    for node in ret.nodes:
        ret.nodes[node]["scrambled"] = node.startswith("scrambled")
    g_scrambled_full = data_utils.get_connected_component_subgraphs(ret)[0]
    print(
        f"The largest connected component that has {g_scrambled_full.number_of_nodes():,} nodes and {g_scrambled_full.number_of_edges():,} edges"
    )
    return ret


g_scrambled_full = get_graph()

# now we will compute the features on the full dataset
fn = "zdf_node_features_full.csv"
if not os.path.exists(fn):
    df_node_features_full = compute_node_features(g_scrambled_full)
    df_node_features_full.reset_index().to_csv(fn)
else:
    df_node_features_full = pd.read_csv(fn).set_index("index")
    df_node_features_full = df_node_features_full[
        [c for c in df_node_features_full.columns if "Unnamed" not in c]
    ].copy()

fn = "zdf_centrality_features_full.csv"
if not os.path.exists(fn):
    df_centrality_features_full = compute_centrality_features(g_scrambled_full)
    df_centrality_features_full.reset_index().to_csv(fn)
else:
    df_centrality_features_full = pd.read_csv(fn).set_index("index")
    df_centrality_features_full = df_centrality_features_full[
        [c for c in df_centrality_features_full.columns if "Unnamed" not in c]
    ].copy()

fn = "zdf_community_features_full.csv"
if not os.path.exists(fn):
    df_community_features_full = compute_community_features(g_scrambled_full)
    df_community_features_full.reset_index().to_csv(fn)
else:
    df_community_features_full = pd.read_csv(fn).set_index("node_id")
    df_community_features_full = df_community_features_full[
        [c for c in df_community_features_full.columns if "Unnamed" not in c]
    ].copy()

fn = "zdf_egograph_features_full.csv"
if not os.path.exists(fn):
    df_egograph_features_full = compute_ego_graph_features(g_scrambled_full)
    df_egograph_features_full.reset_index().to_csv(fn)
else:
    df_egograph_features_full = pd.read_csv(fn).set_index("node_id")
    df_egograph_features_full = df_egograph_features_full[
        [c for c in df_egograph_features_full.columns if "Unnamed" not in c]
    ].copy()

df_all_full = pd.concat(
    [
        df_egograph_features_full,
        df_centrality_features_full,
        df_community_features_full,
    ],
    axis=1,
)

fn = "df_all_features_full.csv"
df_all_full.to_csv(fn)
print(f"Saved full features to {fn}")
