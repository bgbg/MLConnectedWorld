{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Unsupervised Learning Primer",
   "id": "886b44c07e83b50b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This chapter is heavily based on the chapter \"K-Means Clustering\" of \"Programming Collective Intelligence\" by Toby Segaran.\n",
   "id": "51364862691f4bd7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and setup",
   "id": "e1341146f690c893"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import data_utils\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_movies = data_utils.load_dataset_from_local(\"tmdb_5000_movies.csv.zip\")[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"title\",\n",
    "        \"budget\",\n",
    "        \"revenue\",\n",
    "        \"runtime\",\n",
    "        \"popularity\",\n",
    "        \"vote_average\",\n",
    "    ]\n",
    "]\n",
    "df_movies = df_movies.loc[\n",
    "    (df_movies[\"budget\"] > 0) & (df_movies[\"revenue\"] > 0) & (df_movies[\"runtime\"] > 0)\n",
    "].copy()\n",
    "df_movies"
   ],
   "id": "aca48cc17523b413",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clustering items",
   "id": "60904afaa6228aa9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Clustering is a process in which we group a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups. There are many clustering algorithms, such as K-means, DBSCAN, hierarchical clustering, etc. In this chapter, we will focus on the K-means algorithm. The `K-means` algorithm is one of the simplest and most commonly used clustering algorithms. It tries to partition the dataset into `K` pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different as possible.  \n",
    "\n",
    "\n",
    "\n",
    "Eventually, we will use the data above for the clustering, but first, let's examine a simpler example."
   ],
   "id": "ca3b352dee8ab115"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(111)\n",
    "true_center_1 = (0, 0)\n",
    "true_center_2 = (4, 2)\n",
    "x = np.concatenate(\n",
    "    [\n",
    "        np.random.normal(true_center_1[0], 1, 100),\n",
    "        np.random.normal(true_center_2[0], 1, 100),\n",
    "    ]\n",
    ")\n",
    "y = np.concatenate(\n",
    "    [\n",
    "        np.random.normal(true_center_1[1], 1, 100),\n",
    "        np.random.normal(true_center_2[1], 1, 100),\n",
    "    ]\n",
    ")\n",
    "plt.plot(x, y, \".\")"
   ],
   "id": "20bfe3826171537d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The data above is a mix of two distributions. Let's apply K-means clustering to it.\n",
    "\n",
    "We don't have to be super smart to guess that the data above is composed of two clusters, so `K=2` is a good choice for the number of clusters in this case. Later on, we will see how to choose the number of clusters in a more complex case.\n",
    "\n",
    "The K-means algorithm works as follows:\n",
    "- Randomly initialize `K` cluster centers\n",
    "- Assign each point to the nearest cluster center\n",
    "- Recalculate cluster centers\n",
    "- Repeat until convergence\n"
   ],
   "id": "e69746ca6b1f83ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The key, the most important word, in the description above is \"nearest\". It is important because it implies that we need to define a distance metric. There are many distance metrics such as Euclidean, Manhattan, Minkowski, etc. In this case, we will use the Euclidean distance. Select the wrong metric, and you get garbage.",
   "id": "48161dd7f436aa33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rows = np.hstack([x.reshape(-1, 1), y.reshape(-1, 1)])",
   "id": "8a1fea0e761df52b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "k = 2\n",
    "ranges = [\n",
    "    (min([row[i] for row in rows]), max([row[i] for row in rows]))\n",
    "    for i in range(len(rows[0]))\n",
    "]\n",
    "ranges"
   ],
   "id": "beaa4f52a1add3ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clusters = [\n",
    "    [\n",
    "        np.random.random() * (ranges[i][1] - ranges[i][0]) + ranges[i][0]\n",
    "        for i in range(len(rows[0]))\n",
    "    ]\n",
    "    for j in range(k)\n",
    "]\n",
    "clusters"
   ],
   "id": "448efeff8fbb441c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What we did above is to randomly initialize the cluster centers.\n",
    "In theory, we can initialize the cluster centers manually. This is a good idea if we have some prior knowledge about the data or, such as in this case, for demonstration purposes."
   ],
   "id": "242202d1721f1e6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "clusters = [[0, 4], [6, -2]]",
   "id": "b6619c1d596548b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \".\", color=\"gray\")\n",
    "for cluster in clusters:\n",
    "    ax.plot(cluster[0], cluster[1], \"*\", mew=2, ms=12)"
   ],
   "id": "1dac7e21d7310562",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we need to assign each point to the nearest cluster center. To do so, we will first need to decide on the distance metric. In this case, we will use the Euclidean distance.",
   "id": "c44a68b9c27c3c55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def euclidean_distance(v1, v2):\n",
    "    return np.sqrt(sum((v1[i] - v2[i]) ** 2 for i in range(len(v1))))"
   ],
   "id": "fac9d485f90377ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "iterations = 0",
   "id": "ae5a6445babd822b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_matches = [[] for i in range(k)]  # cluster assignments\n",
    "# Find which centroid is the closest for each row\n",
    "for j, row in enumerate(rows):\n",
    "    best_match = 0\n",
    "    for i in range(k):\n",
    "        d = euclidean_distance(clusters[i], row)\n",
    "        if d < euclidean_distance(clusters[best_match], row):\n",
    "            best_match = i\n",
    "    best_matches[best_match].append(j)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(k):\n",
    "    ax.plot(\n",
    "        [rows[j][0] for j in best_matches[i]],\n",
    "        [rows[j][1] for j in best_matches[i]],\n",
    "        \".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "for i, cluster in enumerate(clusters):\n",
    "    ax.plot(cluster[0], cluster[1], \"*\", mew=2, ms=12, color=f\"C{i}\")"
   ],
   "id": "66d0a895b92fe6b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK, so we have the point assignments, now we will update the cluster centers.",
   "id": "8ebac850eaec35ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "old_clusters = np.copy(clusters)\n",
    "for i in range(k):\n",
    "    if best_matches[i]:\n",
    "        avg = np.mean([rows[row_id] for row_id in best_matches[i]], axis=0)\n",
    "        clusters[i] = avg.tolist()\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(k):\n",
    "    ax.plot(\n",
    "        [rows[j][0] for j in best_matches[i]],\n",
    "        [rows[j][1] for j in best_matches[i]],\n",
    "        \".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "for i, cluster in enumerate(clusters):\n",
    "    ax.plot(cluster[0], cluster[1], \"*\", mew=2, ms=12, color=f\"C{i}\")\n",
    "    ax.plot(\n",
    "        [old_clusters[i][0], cluster[0]],\n",
    "        [old_clusters[i][1], cluster[1]],\n",
    "        \"k-\",\n",
    "        zorder=-1,\n",
    "    )"
   ],
   "id": "3bdebcfa3519f150",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Let's repeat the process until convergence",
   "id": "a1371516330b7b22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "last_matches = None\n",
    "for t in range(100):\n",
    "    print(\n",
    "        f\"Iteration {t:03d}. Number of points in cluster 0: {len(best_matches[0])}, cluster 1: {len(best_matches[1])}\"\n",
    "    )\n",
    "    best_matches = [[] for i in range(k)]\n",
    "    # Find which centroid is the closest for each row\n",
    "    for j, row in enumerate(rows):\n",
    "        best_match = 0\n",
    "        for i in range(k):\n",
    "            d = euclidean_distance(clusters[i], row)\n",
    "            if d < euclidean_distance(clusters[best_match], row):\n",
    "                best_match = i\n",
    "        best_matches[best_match].append(j)\n",
    "    # If the results are the same as last time, this is complete\n",
    "    if best_matches == last_matches:\n",
    "        break\n",
    "    last_matches = best_matches\n",
    "    # Move the centroids to the average of their members\n",
    "    for i in range(k):\n",
    "        if best_matches[i]:\n",
    "            avg = np.mean([rows[row_id] for row_id in best_matches[i]], axis=0)\n",
    "            clusters[i] = avg.tolist()\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(k):\n",
    "        ax.plot(\n",
    "            [rows[j][0] for j in best_matches[i]],\n",
    "            [rows[j][1] for j in best_matches[i]],\n",
    "            \".\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        ax.plot(cluster[0], cluster[1], \"x\", mew=3, ms=8, color=f\"C{i}\")"
   ],
   "id": "f6ba357cd0a12e26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from typing import List, Callable, Tuple\n",
    "\n",
    "\n",
    "def k_means(\n",
    "    rows: List[List[float]],\n",
    "    k: int,\n",
    "    distance: Callable[[List[float], List[float]], float],\n",
    ") -> Tuple[List[List[float]], List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Performs K-means clustering on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        rows (List[List[float]]): The dataset, where each element is a list of features for one data point.\n",
    "        k (int): The number of clusters to form.\n",
    "        distance (Callable[[List[float], List[float]], float]): A function to compute the distance between two data points.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[List[float]], List[List[int]]]: A tuple containing the final centroids and the assignment of data points to clusters.\n",
    "    \"\"\"\n",
    "    ranges = [\n",
    "        (min([row[i] for row in rows]), max([row[i] for row in rows]))\n",
    "        for i in range(len(rows[0]))\n",
    "    ]\n",
    "    clusters = [\n",
    "        [\n",
    "            np.random.random() * (ranges[i][1] - ranges[i][0]) + ranges[i][0]\n",
    "            for i in range(len(rows[0]))\n",
    "        ]\n",
    "        for j in range(k)\n",
    "    ]\n",
    "\n",
    "    last_matches = None\n",
    "    for t in range(100):\n",
    "        best_matches = [[] for i in range(k)]\n",
    "\n",
    "        # Find which centroid is the closest for each row\n",
    "        for j, row in enumerate(rows):\n",
    "            best_match = 0\n",
    "            for i in range(k):\n",
    "                d = distance(clusters[i], row)\n",
    "                if d < distance(clusters[best_match], row):\n",
    "                    best_match = i\n",
    "            best_matches[best_match].append(j)\n",
    "\n",
    "        # If the results are the same as last time, this is complete\n",
    "        if best_matches == last_matches:\n",
    "            break\n",
    "        last_matches = best_matches\n",
    "\n",
    "        # Move the centroids to the average of their members\n",
    "        for i in range(k):\n",
    "            if best_matches[i]:\n",
    "                avg = np.mean([rows[row_id] for row_id in best_matches[i]], axis=0)\n",
    "                clusters[i] = avg.tolist()\n",
    "\n",
    "    return clusters, best_matches"
   ],
   "id": "86805b24e5ea91e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clusters, best_matches = k_means(rows, 2, euclidean_distance)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(k):\n",
    "    ax.plot(\n",
    "        [rows[j][0] for j in best_matches[i]],\n",
    "        [rows[j][1] for j in best_matches[i]],\n",
    "        \".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "for i, cluster in enumerate(clusters):\n",
    "    ax.plot(cluster[0], cluster[1], \"x\", mew=3, ms=8, color=f\"C{i}\")"
   ],
   "id": "d63c29029049323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Unsupervised learning assumes that the ground truth isn't known. However, in this synthetic example, we know that the data is composed of two clusters. We also know the true centroids. Let's compare the true centroids with the centroids we found.",
   "id": "1aa507ac376ea41e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \".\", color=\"gray\")\n",
    "for i, cluster in enumerate(clusters):\n",
    "    ax.plot(cluster[0], cluster[1], \"x\", mew=3, ms=8, color=f\"C{i}\")\n",
    "ax.plot(true_center_1[0], true_center_1[1], \"o\", mew=2, ms=12, mec=f\"C{0}\", mfc=\"none\")\n",
    "ax.plot(true_center_2[0], true_center_2[1], \"o\", mew=2, ms=12, mec=f\"C{1}\", mfc=\"none\")"
   ],
   "id": "c806d316a5c1c1a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Like we said, before running the clustering, we need to decide how we compute the distance between two points. In the example above, we used the Euclidean distance. Let's try the [Manhattan distance now](https://en.wikipedia.org/wiki/Taxicab_geometry).",
   "id": "4de573f8bd74553b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def manhattan_distance(v1, v2):\n",
    "    return sum(abs(v1[i] - v2[i]) for i in range(len(v1)))\n",
    "\n",
    "\n",
    "clusters, best_matches = k_means(rows, 2, manhattan_distance)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(k):\n",
    "    ax.plot(\n",
    "        [rows[j][0] for j in best_matches[i]],\n",
    "        [rows[j][1] for j in best_matches[i]],\n",
    "        \".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "for i, cluster in enumerate(clusters):\n",
    "    ax.plot(cluster[0], cluster[1], \"x\", mew=3, ms=8, color=f\"C{i}\")"
   ],
   "id": "bacdfa83917ec629",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cosine distance",
   "id": "4a296fd90de40af5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cosine_distance(v1, v2):\n",
    "    return 1 - np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "clusters, best_matches = k_means(rows, 2, cosine_distance)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(k):\n",
    "    ax.plot(\n",
    "        [rows[j][0] for j in best_matches[i]],\n",
    "        [rows[j][1] for j in best_matches[i]],\n",
    "        \".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "for i, cluster in enumerate(clusters):\n",
    "    ax.plot(cluster[0], cluster[1], \"x\", mew=3, ms=8, color=f\"C{i}\")"
   ],
   "id": "72d50b185e5785ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The fact that you can use a certain distance metric doesn't mean you **should** use it. The choice of the distance metric is crucial and can make or break your clustering. In the example above, the Manhattan distance can be a good choice if the data points represent instances on a grid that can only be accessed by traveling along the grid. I can't think of a valid reason to use the cosine distance in this case, but it's a good example to show that you can use it.",
   "id": "b617b689294054c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Of course, we are not savages and don't have to implement K-means from scratch every time we need to use it. There are many libraries that implement it, such as `scikit-learn` which provides very efficient implementations of many machine learning algorithms. ",
   "id": "99b47086196c71c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sklearn.cluster\n",
    "\n",
    "[a for a in dir(sklearn.cluster) if isinstance(getattr(sklearn.cluster, a), type)]"
   ],
   "id": "70f626088a2db206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(rows)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(kmeans.n_clusters):\n",
    "    ax.plot(\n",
    "        [rows[j][0] for j in range(len(rows)) if kmeans.labels_[j] == i],\n",
    "        [rows[j][1] for j in range(len(rows)) if kmeans.labels_[j] == i],\n",
    "        \".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "for i, cluster in enumerate(kmeans.cluster_centers_):\n",
    "    ax.plot(cluster[0], cluster[1], \"x\", mew=3, ms=8, color=f\"C{i}\")"
   ],
   "id": "afd0b53fc2066f93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The only problem with sklearn's KMeans is that it does not allow us to easily switch the distance metric. The library [NLTK](https://www.nltk.org/) is a natural language processing library that provides a KMeans implementation that allows us to switch the distance metric. We will not use it in this example, but it's good to know that it exists.",
   "id": "101b5e99e4fbc392"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Number of clusters\n",
    "\n",
    "How do we decide the number of clusters? In the example above, we knew that the data was composed of two clusters. In real-world scenarios, we don't know the number of clusters. There are many methods to determine the number of clusters, such as the elbow method, the silhouette score, etc. We will use the elbow method as an example.\n",
    "\n",
    "The central concept in this method is within-cluster sum of squares (WCSS). This metric measures the compactness of the clusters. The idea is to plot the WCSS against the number of clusters. The point where the curve starts to flatten is the elbow point, which is the optimal number of clusters."
   ],
   "id": "a664dc48445b9958"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_wcss(data: np.ndarray, kmeans: KMeans) -> float:\n",
    "    \"\"\"\n",
    "    Compute the within-cluster sum of squares (WCSS) for a given KMeans model.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The dataset.\n",
    "        kmeans (KMeans): The KMeans model after fitting.\n",
    "\n",
    "    Returns:\n",
    "        float: The WCSS value.\n",
    "    \"\"\"\n",
    "    wcss = 0.0\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        cluster_points = data[kmeans.labels_ == i]\n",
    "        centroid = kmeans.cluster_centers_[i]\n",
    "        wcss += np.sum((cluster_points - centroid) ** 2)\n",
    "    return wcss"
   ],
   "id": "6d53dbe3d3841947",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(rows)\n",
    "    wcss.append((k, compute_wcss(rows, kmeans)))\n",
    "wcss = np.array(wcss)\n",
    "# Plot the WCSS values\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(wcss[:, 0], wcss[:, 1], marker=\"o\")\n",
    "ax.text(wcss[1, 0], wcss[1, 1], \" elbow point\", fontsize=12, verticalalignment=\"bottom\")"
   ],
   "id": "dbce438d2db65511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the two cells of code above, you saw attributes of the k-means object with a trailing underscore (`kmeans.cluster_centers_` and `kmeans.labels_`). In `sklearn` ecosystem, attributes with a trailing underscore are set after the model is fitted and provide additional information about the model. For example, `kmeans.cluster_centers_` contains the cluster centers after the model is fitted.\n",
    "\n",
    "One such an additional attribute is `inertia_`, which is the WCSS. The code below is equivalent to the code above."
   ],
   "id": "a83bd29a447a6dce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(rows)\n",
    "    wcss.append((k, kmeans.inertia_))\n",
    "wcss = np.array(wcss)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(wcss[:, 0], wcss[:, 1], marker=\"o\")"
   ],
   "id": "53fe57f9546cffe0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clustering real-life data\n",
    "Let's now apply K-means clustering to the movie dataset we loaded at the beginning of this section."
   ],
   "id": "1eecb7c8e2d8a7e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_movies.head()",
   "id": "d11547710adf649d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Recall the importance of distance metrics. By the nature of the Euclidian distance, the algorithm will be biased towards features with higher values. In our case, the budget and the revenue have the order of magnitude of millions, while the vote average is between 0 and 10. To avoid this bias, we will normalize the data. The standard way to do so is to subtract the mean and divide by the standard deviation. Additionally, we will transform the budget and revenue to log scale before normalization.",
   "id": "f9fcc8bba4caeb53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_scaled = df_movies.drop(columns=\"id\").set_index(\"title\").copy()\n",
    "df_scaled[\"budget\"] = np.log1p(df_scaled[\"budget\"])\n",
    "df_scaled[\"revenue\"] = np.log1p(df_scaled[\"revenue\"])\n",
    "for col in df_scaled.columns:\n",
    "    df_scaled[col] = (df_scaled[col] - df_scaled[col].mean()) / df_scaled[col].std()\n",
    "df_scaled.head()"
   ],
   "id": "6799a504b784b97a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wcss = []\n",
    "for k in range(1, 21):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    wcss.append((k, kmeans.inertia_))\n",
    "wcss = np.array(wcss)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(wcss[:, 0], wcss[:, 1], marker=\"o\")"
   ],
   "id": "bf2dc18feffe0013",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Unsurprisingly, there is no clear elbow point in the plot above. This is an indication that the clustering movies using the features we have is not expected to provide meaningful clusters. If we really need to cluster movies, we should use more features, such as genres, keywords, etc.",
   "id": "580d68642ac8bc9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9e69e0622facdc32",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
